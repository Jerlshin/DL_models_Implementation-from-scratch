{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, einsum\n",
    "import numpy as np \n",
    "from einops import rearrange\n",
    "import torch.nn.functional as f\n",
    "import plotly.graph_objs as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Merging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Code for the Patch Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# only for STB1, they have used \"Patch Merging\" and \"Linear Embedding\" and have not used \"Patch Merging\"\n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        # patch merging is just used for downscaling the feature set ig \n",
    "        \n",
    "        super().__init__()\n",
    "        # just changing the feature size without overlaping and kernel size is the window size \n",
    "        # kernel should not overlap with each other \n",
    "        self.patch_merge = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=downscaling_factor,  # this is fine. Mostly the size of the window\n",
    "            stride=downscaling_factor, # no window overlappting, so the stride should shift accordingly \n",
    "            # so, we would have more control to change the saliency feature of the iamge  \n",
    "            padding=0\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.patch_merge(x).permute(0, 2, 3, 1) # shift the channel to the last\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[[[ 154.0996,  -29.3429, -217.8789],\n",
      "          [  56.8431, -108.4522, -139.8595]],\n",
      "\n",
      "         [[  40.3347,   83.8026,  -71.9258],\n",
      "          [ -40.3344,  -59.6635,   18.2036]]]])\n",
      "Output:  tensor([[[[ 1.2191,  0.0112, -1.2303],\n",
      "          [ 1.3985, -0.5173, -0.8813]],\n",
      "\n",
      "         [[ 0.3495,  1.0120, -1.3615],\n",
      "          [-0.3948, -0.9787,  1.3735]]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "B, H, W, C = 1, 2, 2, 3  # 4 tokens across 3 channels \n",
    "input = torch.randn(B, H, W, C) * 100 # for better illustration\n",
    "print(\"Input: \", input)\n",
    "layer_norm = nn.LayerNorm(C)  # give no of channels as the dim \n",
    "output = layer_norm(input)\n",
    "print(\"Output: \", output)\n",
    "\n",
    "# each token of the 4 tokens normalized with respect to iteself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normalize each token with respect to iteself\n",
    "\"\"\"\n",
    "class PreNorm(nn.Module):  # Layer Normalization. \n",
    "    # normalize the input before sending to the WindowAttention or FeedForward \n",
    "    def __init__(self, dim, fn):   # fn - WindowAttention() as the function input or block \n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)  # number of channels as the input, normalizing across channels\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs) # normalize and send to the block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)  # number of channels as the input, normalizing across channels\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        # apply normalization after the processing \n",
    "        return self.norm(self.fn(x, **kwargs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    # fn -- PreNorm(WindowAttention())   this is the function input \n",
    "    def __init__(self, fn):  # it is the function or the block \n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    # X - input \n",
    "    def forward(self, x, **kwargs):  # **kwargs -- params \n",
    "        return self.fn(x, **kwargs) + x   # add the input of the block to the output of the block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
      "        15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.,\n",
      "        29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.,\n",
      "        43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55., 56.,\n",
      "        57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70.,\n",
      "        71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81.])\n",
      "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14., 15., 16., 17., 18.],\n",
      "        [19., 20., 21., 22., 23., 24., 25., 26., 27.],\n",
      "        [28., 29., 30., 31., 32., 33., 34., 35., 36.],\n",
      "        [37., 38., 39., 40., 41., 42., 43., 44., 45.],\n",
      "        [46., 47., 48., 49., 50., 51., 52., 53., 54.],\n",
      "        [55., 56., 57., 58., 59., 60., 61., 62., 63.],\n",
      "        [64., 65., 66., 67., 68., 69., 70., 71., 72.],\n",
      "        [73., 74., 75., 76., 77., 78., 79., 80., 81.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(1, 81, 81).view(9, 9)  # 9 * 9 is 81, so, we will get a sqaure tensor \n",
    "print(torch.linspace(1, 81, 81))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[81., 73., 74., 75., 76., 77., 78., 79., 80.],\n",
      "        [ 9.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
      "        [18., 10., 11., 12., 13., 14., 15., 16., 17.],\n",
      "        [27., 19., 20., 21., 22., 23., 24., 25., 26.],\n",
      "        [36., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
      "        [45., 37., 38., 39., 40., 41., 42., 43., 44.],\n",
      "        [54., 46., 47., 48., 49., 50., 51., 52., 53.],\n",
      "        [63., 55., 56., 57., 58., 59., 60., 61., 62.],\n",
      "        [72., 64., 65., 66., 67., 68., 69., 70., 71.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.roll(input=x, shifts=(1, 1), dims=(0, 1))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11., 12., 13., 14., 15., 16., 17., 18., 10.],\n",
      "        [20., 21., 22., 23., 24., 25., 26., 27., 19.],\n",
      "        [29., 30., 31., 32., 33., 34., 35., 36., 28.],\n",
      "        [38., 39., 40., 41., 42., 43., 44., 45., 37.],\n",
      "        [47., 48., 49., 50., 51., 52., 53., 54., 46.],\n",
      "        [56., 57., 58., 59., 60., 61., 62., 63., 55.],\n",
      "        [65., 66., 67., 68., 69., 70., 71., 72., 64.],\n",
      "        [74., 75., 76., 77., 78., 79., 80., 81., 73.],\n",
      "        [ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.roll(input=x, shifts=(-1, -1), dims=(0, 1))  # dims is h and w \n",
    "print(y)\n",
    "\n",
    "# so, last row and last col window will have unrelated pixels as it came from the other side \n",
    "# we don't want this\n",
    "# so, we apply masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to mask the last row and last column window \n",
    "# 8 -> 4 -> 2 -> 1\n",
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "    \n",
    "    # as this is (-displacement), it would be like shift left and up. \n",
    "    def forward(self, x):\n",
    "        return torch.roll(x,shifts=(self.displacement, self.displacement), dims=(1, 2)) # roll to the right and the down. \n",
    "    # we have 3 dims, that is h x w x c    # but, this will be changed to c x h x w \n",
    "    # so, we are shifting dims in the 1 and 2, understood clearly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for each of the patches (windows) \n",
    "# we are working on pixels \n",
    "def create_mask(  # to handle cycleshifted patches \n",
    "    window_size, \n",
    "    displacement, \n",
    "    upper_lower, \n",
    "    left_right\n",
    "):\n",
    "    # make a matrix mask for the left and right based on the condition\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)   # (49, 49)   # window size is 7 so, it would be 49 x 49\n",
    "    # print('Original mask: \\n', mask)\n",
    "    \n",
    "    if upper_lower:  # displacement = window_size // 2\n",
    "        # [h, w]\n",
    "        # from down to up from the lower, \n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf') # down left section \n",
    "        mask[:-displacement * window_size:, -displacement * window_size:] = float('-inf') # up right section\n",
    "    \n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "    \n",
    "    # print(\"Processed Mask: \\n\", mask)\n",
    "    return mask    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "displacement = window_size // 2\n",
    "\n",
    "mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "print(mask)\n",
    "print(displacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "upper_lower = True\n",
    "\n",
    "mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "if upper_lower:  \n",
    "    # down left section \n",
    "    # displacement * window_size\n",
    "    mask[-displacement * window_size:, :-displacement * window_size] = float(1) \n",
    "    mask[:-displacement * window_size:, -displacement * window_size:] = float(1) \n",
    "\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 0., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "left_right = True\n",
    "\n",
    "mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "if left_right:\n",
    "    mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "    mask[:, -displacement:, :, :-displacement] = float(1)\n",
    "    mask[:, :-displacement, :, -displacement] = float(1)\n",
    "    mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "pattern = mask[:3, :3]\n",
    "print(pattern)  # this pattern keeps on repeating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.47 -0.16  1.44  0.27  0.17  0.87 -0.14 -0.11  0.93]\n",
      " [ 1.26  2.    0.05  0.62 -0.41 -0.84 -2.32 -0.22 -0.74]\n",
      " [ 0.56  0.26 -0.17 -0.68  0.94  0.49  1.2   0.08 -1.2 ]\n",
      " [-0.   -0.52 -0.31 -1.58  1.71  0.21 -0.45 -0.57 -0.56]\n",
      " [ 0.59  1.54  0.51 -0.59 -1.33  0.19 -0.07 -0.49 -1.5 ]\n",
      " [-0.19  0.45  1.33  1.51  2.08  1.71  2.38 -1.13 -0.32]\n",
      " [-1.09 -0.09  0.33 -0.76 -1.6   0.02 -0.75  0.19  0.62]\n",
      " [ 0.64 -0.    1.11  0.28  0.43 -0.8  -1.3  -0.75 -1.31]\n",
      " [-0.22 -0.33 -0.43  0.23  0.8  -0.18 -0.37 -1.21 -0.62]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109223/1867118211.py:10: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Abs Pos Embedding\"\"\"\n",
    "\n",
    "window_size = 3 # 9 elements\n",
    "num_of_params = 81  # size of the first stage,  # 9 * 9 = 81 \n",
    "# relate each param to one other \n",
    "pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2), requires_grad=False)\n",
    "\n",
    "# print(torch.tensor(pos_embedding).apply_(lambda x: float(f\"{x:.2f}\")))\n",
    "\n",
    "print(torch.tensor(pos_embedding).clone().detach().numpy())\n",
    "\n",
    "# print(torch.round(torch.tensor(pos_embedding) * 100) / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative Positional Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this case, in total, it would be 25 x 25 param for the RPE. but in abs position, i had 81 x 81 as each element should look to other 8 elements including itself. so, it would be 9. so, for 9 elements, it would be 81 x 81 elements as 9 elements on each row and column \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SWIN transformer case, \n",
    "\n",
    "window_size of 7 would result in parameter size of 13 x 13. \n",
    "\n",
    "It means that, each window will look 6 elements in the right and 6 elements in the left and 1 itself. so, 2M-1 where M is the window_size  and same goes for up and down so another 2M-1 \n",
    "\n",
    "so, the resulting would be (2M-1)x(2M-1) -- this is just the parameter and no the matrix we need ofcourse \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.0367, -0.6037, -1.2788,  0.0930, -0.6661],\n",
      "        [ 0.6080, -0.7300, -0.8834,  0.6596,  0.2440],\n",
      "        [ 1.1646,  0.2886,  0.3866, -0.2011, -0.1179],\n",
      "        [-0.8294, -1.4073, -1.9003,  0.1307, -0.7043],\n",
      "        [ 0.3147,  0.1574,  0.3854,  0.5737,  0.9979]])\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "# 9 x 9 parameters\n",
    "\"\"\"It has 2 channels\"\"\"\n",
    "\n",
    "# (2 * 3 -1) x (2 * 3 - 1) = 5 x 5 = 25 params, but 2 channels \n",
    "pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1), requires_grad=False)\n",
    "print(pos_embedding)\n",
    "\n",
    "# for a window size = 3, so, the matrix for position embedding would be 9 x 9\n",
    "# the set of 3 x 3 in 9 x 9 matrix will be repeating itself throughout the matrix \n",
    "# we count only the original, so, in each 3 x 3 matrix, we have 5 original elements \n",
    "# and across the 9 x 9 matrix, we would have 5 set os 5 elements \n",
    "# so 25 parameters will for relative position  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4, 1)\n",
      "(1, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4])\n",
    "print(arr.shape)\n",
    "arr = arr[:, None]\n",
    "print(arr.shape)\n",
    "arr = arr[None, :]\n",
    "print(arr.shape)\n",
    "\n",
    "# None adds the new dim at the specified position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 2],\n",
      "        [2, 0],\n",
      "        [2, 1],\n",
      "        [2, 2]])\n",
      "torch.Size([9, 2])\n",
      "torch.Size([9, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))  # it will have 3*3 = 9 indices \n",
    "print(indices)\n",
    "print(indices.shape)\n",
    "\n",
    "distances = indices[None, :, :] - indices[:, None, :]\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           0,
           0,
           0,
           1,
           0,
           2,
           1,
           0,
           1,
           1,
           1,
           2,
           2,
           0,
           2,
           1,
           2,
           2,
           0,
           -1,
           0,
           0,
           0,
           1,
           1,
           -1,
           1,
           0,
           1,
           1,
           2,
           -1,
           2,
           0,
           2,
           1,
           0,
           -2,
           0,
           -1,
           0,
           0,
           1,
           -2,
           1,
           -1,
           1,
           0,
           2,
           -2,
           2,
           -1,
           2,
           0,
           -1,
           0,
           -1,
           1,
           -1,
           2,
           0,
           0,
           0,
           1,
           0,
           2,
           1,
           0,
           1,
           1,
           1,
           2,
           -1,
           -1,
           -1,
           0,
           -1,
           1,
           0,
           -1,
           0,
           0,
           0,
           1,
           1,
           -1,
           1,
           0,
           1,
           1,
           -1,
           -2,
           -1,
           -1,
           -1,
           0,
           0,
           -2,
           0,
           -1,
           0,
           0,
           1,
           -2,
           1,
           -1,
           1,
           0,
           -2,
           0,
           -2,
           1,
           -2,
           2,
           -1,
           0,
           -1,
           1,
           -1,
           2,
           0,
           0,
           0,
           1,
           0,
           2,
           -2,
           -1,
           -2,
           0,
           -2,
           1,
           -1,
           -1,
           -1,
           0,
           -1,
           1,
           0,
           -1,
           0,
           0,
           0,
           1,
           -2,
           -2,
           -2,
           -1,
           -2,
           0,
           -1,
           -2,
           -1,
           -1,
           -1,
           0,
           0,
           -2,
           0,
           -1,
           0,
           0
          ],
          "colorscale": [
           [
            0,
            "#440154"
           ],
           [
            0.1111111111111111,
            "#482878"
           ],
           [
            0.2222222222222222,
            "#3e4989"
           ],
           [
            0.3333333333333333,
            "#31688e"
           ],
           [
            0.4444444444444444,
            "#26828e"
           ],
           [
            0.5555555555555556,
            "#1f9e89"
           ],
           [
            0.6666666666666666,
            "#35b779"
           ],
           [
            0.7777777777777778,
            "#6ece58"
           ],
           [
            0.8888888888888888,
            "#b5de2b"
           ],
           [
            1,
            "#fde725"
           ]
          ],
          "size": 5
         },
         "mode": "markers",
         "type": "scatter3d",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8
         ],
         "y": [
          0,
          0,
          1,
          1,
          2,
          2,
          3,
          3,
          4,
          4,
          5,
          5,
          6,
          6,
          7,
          7,
          8,
          8,
          0,
          0,
          1,
          1,
          2,
          2,
          3,
          3,
          4,
          4,
          5,
          5,
          6,
          6,
          7,
          7,
          8,
          8,
          0,
          0,
          1,
          1,
          2,
          2,
          3,
          3,
          4,
          4,
          5,
          5,
          6,
          6,
          7,
          7,
          8,
          8,
          0,
          0,
          1,
          1,
          2,
          2,
          3,
          3,
          4,
          4,
          5,
          5,
          6,
          6,
          7,
          7,
          8,
          8,
          0,
          0,
          1,
          1,
          2,
          2,
          3,
          3,
          4,
          4,
          5,
          5,
          6,
          6,
          7,
          7,
          8,
          8,
          0,
          0,
          1,
          1,
          2,
          2,
          3,
          3,
          4,
          4,
          5,
          5,
          6,
          6,
          7,
          7,
          8,
          8,
          0,
          0,
          1,
          1,
          2,
          2,
          3,
          3,
          4,
          4,
          5,
          5,
          6,
          6,
          7,
          7,
          8,
          8,
          0,
          0,
          1,
          1,
          2,
          2,
          3,
          3,
          4,
          4,
          5,
          5,
          6,
          6,
          7,
          7,
          8,
          8,
          0,
          0,
          1,
          1,
          2,
          2,
          3,
          3,
          4,
          4,
          5,
          5,
          6,
          6,
          7,
          7,
          8,
          8
         ],
         "z": [
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1
         ]
        }
       ],
       "layout": {
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 0
        },
        "scene": {
         "xaxis": {
          "title": {
           "text": "X"
          }
         },
         "yaxis": {
          "title": {
           "text": "Y"
          }
         },
         "zaxis": {
          "title": {
           "text": "Z"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y, z = np.indices(distances.shape)\n",
    "scatter = go.Scatter3d(x=x.flatten(), y=y.flatten(), z=z.flatten(),\n",
    "                       mode='markers',\n",
    "                       marker=dict(size=5, color=distances.flatten(), colorscale='Viridis'))\n",
    "\n",
    "# Set up the layout\n",
    "layout = go.Layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n",
    "                   margin=dict(l=0, r=0, b=0, t=0))\n",
    "\n",
    "fig = go.Figure(data=[scatter], layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
       "        [ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
       "        [ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
       "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
       "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
       "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
       "        [-2, -2, -2, -1, -1, -1,  0,  0,  0],\n",
       "        [-2, -2, -2, -1, -1, -1,  0,  0,  0],\n",
       "        [-2, -2, -2, -1, -1, -1,  0,  0,  0]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[:, :, 0]  # so, 5 different sub-matrixes you see that you ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [-1,  0,  1],\n",
       "        [-2, -1,  0]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[:3, :3, 1]  # this keeps on repeating itself for each of the 3 x 3 sub matrix in the whole matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  0,  1,  2,  0,  1,  2],\n",
       "        [-1,  0,  1, -1,  0,  1, -1,  0,  1],\n",
       "        [-2, -1,  0, -2, -1,  0, -2, -1,  0],\n",
       "        [ 0,  1,  2,  0,  1,  2,  0,  1,  2],\n",
       "        [-1,  0,  1, -1,  0,  1, -1,  0,  1],\n",
       "        [-2, -1,  0, -2, -1,  0, -2, -1,  0],\n",
       "        [ 0,  1,  2,  0,  1,  2,  0,  1,  2],\n",
       "        [-1,  0,  1, -1,  0,  1, -1,  0,  1],\n",
       "        [-2, -1,  0, -2, -1,  0, -2, -1,  0]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[:, :, 1]  # each matrix of 3 x 3 is repeating itself. \n",
    "# if you notice, that matrix has 5 unique elements\n",
    "# so, we have 5 x 5 (from the 0th channel and the 1st channel), so, we have 25 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 2]), torch.Size([9, 1, 2]))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[None, :, :].shape , indices[:, None, :].shape\n",
    "\n",
    "# basically, we are making a row of the indices and then the columns of the indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 9, 2])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
      "        [ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
      "        [ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
      "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
      "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
      "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
      "        [-2, -2, -2, -1, -1, -1,  0,  0,  0],\n",
      "        [-2, -2, -2, -1, -1, -1,  0,  0,  0],\n",
      "        [-2, -2, -2, -1, -1, -1,  0,  0,  0]]) \n",
      "\n",
      " tensor([[ 0,  1,  2,  0,  1,  2,  0,  1,  2],\n",
      "        [-1,  0,  1, -1,  0,  1, -1,  0,  1],\n",
      "        [-2, -1,  0, -2, -1,  0, -2, -1,  0],\n",
      "        [ 0,  1,  2,  0,  1,  2,  0,  1,  2],\n",
      "        [-1,  0,  1, -1,  0,  1, -1,  0,  1],\n",
      "        [-2, -1,  0, -2, -1,  0, -2, -1,  0],\n",
      "        [ 0,  1,  2,  0,  1,  2,  0,  1,  2],\n",
      "        [-1,  0,  1, -1,  0,  1, -1,  0,  1],\n",
      "        [-2, -1,  0, -2, -1,  0, -2, -1,  0]])\n"
     ]
    }
   ],
   "source": [
    "dummy = indices[None, :, :] - indices[:, None, :] \n",
    "print(dummy.shape)\n",
    "print(dummy[:, :, 0], \"\\n\\n\", dummy[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : \n",
      " tensor([1, 2, 3]) torch.Size([3])\n",
      "a1: \n",
      " tensor([[1, 2, 3]]) torch.Size([1, 3])\n",
      "a2: \n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]]) torch.Size([3, 1])\n",
      "d : \n",
      " tensor([[ 0,  1,  2],\n",
      "        [-1,  0,  1],\n",
      "        [-2, -1,  0]]) torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "print('a : \\n', a, a.shape)\n",
    "a1 = a[None, :]\n",
    "print('a1: \\n', a1, a1.shape)  \n",
    "a2 = a[:, None]\n",
    "print('a2: \\n', a2, a2.shape)\n",
    "d = a1 - a2 \n",
    "print('d : \\n', d, d.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array boardcasting if operations on arrays with different dimensions\n",
    "\n",
    "A sensible way of doing element-wise operations on arrays of different (but compatible) shapes \n",
    "\n",
    "For the given tensor or array, broadcasting can be done with any number of dimesions with dim=1 it has. Simply, shape of 3 could be 1x3 or 1x1x3 or 1x3x1 or so on\n",
    "\n",
    "1 can match with any dimensions given\n",
    "\n",
    "If shorter, prepend 1 with the dimensions. only prepending. \n",
    "\n",
    "- when matching the exact dim, just atleast one has to be 1 to make a match, and you can prepend if you want. \n",
    "- the results, would be the number other than the dim 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_distances(window_size):\n",
    "    # this is creating all the indices i need for the rows and columns. \n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)])) # indices of each and every elements of the matrix \n",
    "    # print(\"Indices for the Relative Distance: \\n\", indices.size())\n",
    "    \n",
    "    \n",
    "    \"\"\"Array boardcasting\"\"\"\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)\n",
    "\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        nw_h = n_h // self.window_size\n",
    "        nw_w = n_w // self.window_size\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention_(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim,   # no of channels \n",
    "        heads,  # heads = num_heads = (3, 6, 12, 24)\n",
    "        head_dim, \n",
    "        shifted,  # for SW-MSA \n",
    "        window_size,   # window size is 7, as the size of the last block is 7 x 7, so, we divide every feature before that in patches of 7 window size \n",
    "        relative_pos_embedding,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"hidden_dim is actually the number of channels technically\"\"\"\n",
    "        # dim = hidden_dim = (96, 192, 384, 768)  # out_channels from each block \n",
    "                \n",
    "        # head_dim = 32 \n",
    "        \n",
    "        \"\"\"\n",
    "        head * head_dim = channels \n",
    "        \n",
    "        (3 * 32) = 96\n",
    "        (6 * 32) = 192\n",
    "        (12 * 32) = 384\n",
    "        (24 * 32) = 768\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        inner_dim = head_dim * heads   # (channels calculating)\n",
    "        self.heads = heads \n",
    "        \n",
    "        # for the Attention(Q, K, V) = Softmax(QK^T / sqrt(head_dim) +B)V  # what is B --> constant for regularization\n",
    "        self.scale = head_dim ** -0.5  # 1 / sqrt(head_dim)\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "        \n",
    "        # to make the connections between multiple windows by shifting and padding those \n",
    "        if self.shifted:\n",
    "            # shit all the windows to right and down and pad them by half of the size of the window to make connections between windows \n",
    "            # and padd the empty space: \n",
    "            \"\"\"\n",
    "            2 padding: \n",
    "                Naive padding (adding 0)\n",
    "                Cyclic padding  -- [faster] \n",
    "            \"\"\"\n",
    "            displacement = window_size // 2  # half of the window size \n",
    "            \n",
    "            # shift them and should be able to shift them back how it ws before \n",
    "            \"\"\"\n",
    "            With the cyclic shift, the no of padding remains the same, and we don't need any extra computations to process the padded 0 as in the naive solution\n",
    "            \"\"\"\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            \n",
    "            # problem of last row and last column would be in problem with this Cyclic Shift \n",
    "            # when we shift to right and down. \n",
    "            \"\"\"Windows at last row and last column would be related to unrelated patches from the other side. we don't want this\"\"\"\n",
    "            # create a matrix to mask. 0-no mask 1-mask (-infinity). make a matrix of n**2 x n**2 and then make the masking. \n",
    "            \n",
    "            # masks are NOT learnable params, requires_grad=False\n",
    "            # Last row matrix \n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            \n",
    "            # last column matrix (COMPLEX.. couldn't understand)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,                                                             \n",
    "                                                            upper_lower=False,  left_right=True), requires_grad=False)\n",
    "            \n",
    "        \n",
    "        # query, key, value. \n",
    "        # increase the output by 3\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)  # inner_dim is the channels of the current layer \n",
    "        \n",
    "        # positional embedding \n",
    "        \"\"\"Absolute positional embedding\"\"\"\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2)) # (49 x 49)\n",
    "        \n",
    "        \"\"\"Relative Positional embedding\"\"\"\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1  # this will create a big matrix where we will put matrix inside that      \n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))  # 13 x 13 parameters --- 2 * 7 - 1   \n",
    "        else:\n",
    "            # normal positional embedding\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))         \n",
    "        \"\"\"ViT paper:\n",
    "        we divide image into pathces and each patch is itself a token. \n",
    "        # where the patch belongs - pos embedding \n",
    "        \"\"\"\n",
    "        \n",
    "        self.tau = nn.Parameter(torch.tensor(0.01), requires_grad=True) # learnable parameter. Initialized to 0.01 initialy according to the paper \n",
    "        \n",
    "        # first normalizing q and k with respect to each row \n",
    "        \n",
    "        self.to_out = nn.Linear(inner_dim, dim)  # inner_dim = channels \n",
    "        # dim = hidden_dim = channels \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)  # cyclic shift won't change the dimensions \n",
    "            # we need to use masking technique if we are using cyclic shift.\n",
    "            # print(x.size())\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads  # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
    "        \n",
    "        # print(self.to_qkv(x).size()) # (1, (56, 28, 14, 7), (56, 28, 14, 7), (288, 576, 1152, 2304))\n",
    "        # channels are just changed, h w don't change\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1) # across channels \n",
    "        # q k v will have the same size \n",
    "        \n",
    "        # rows and cols of windows \n",
    "        # no of windows i have in each stage  \n",
    "        nw_h = n_h // self.window_size   # 8, 4, 2, 1  for all 4 stages \n",
    "        nw_w = n_w // self.window_size\n",
    "        \n",
    "        # using rearrage from the einops \n",
    "        # head in first dim = 3\n",
    "        # head_dim = 32 \n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t,\n",
    "                                # b, h=#heads, (nw_h*nw_w) = (64, 16, 4, 1), (w_h, w_w)=(7 * 7 = 49), d=32  \n",
    "                                'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',  # if in brackets, it will multiply \n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size),  # decomposition. adding new dim, so epcify\n",
    "            qkv\n",
    "        )\n",
    "        \n",
    "        # print(q.size()) # k and v would have the same shape \n",
    "        # (b=1, h=(3, 6, 12, 24), (nw_h * nw_w)=(74, 16, 4, 1), (w_h * w_w) = 49, d=32)\n",
    "        \n",
    "        # QK_t  \"\"\" i j -- window size (49 pixels, (7 x 7))\"\n",
    "        # h = #heads (3, 6, 12, 24) \n",
    "        # w = (64, 16, 4, 1) # how the pixels are grouped into window_size and grouped \n",
    "        # DOT PRODUCT SIMILARITY \n",
    "        # dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale  # attention formula \n",
    "        # h - no of heads  \n",
    "        # w - no of windows \n",
    "        # i, j - 49 (7 * 7)  #  \n",
    "        # d = 32 # head_dim \n",
    "        \n",
    "        \"\"\"Cosine similiarity\"\"\"\n",
    "        # sim(q_i, k_j) = cos(q_i, k_j) / t_ + B_ij      # t_ is the tou which is the learnable paramter \n",
    "        # B_ij is the relative positional bias between pixel i and j. \n",
    "        \n",
    "        # eulidean norm \n",
    "        # to normalize a vector, we divide each element of the vector by the L2 norm  \n",
    "        q = f.normalize(q, p=2, dim=-1) # L2norm as p=2 ... ensuring that each vector  has unit length\n",
    "        k = f.normalize(k, p=2, dim=-1)  # d = 32 # normalize along the d=32 \n",
    "        \n",
    "        # cosine similarity \n",
    "        # if you multiply v by the reciprocal of its length ||V||_2 you are essentially tranforming the vector to have a magnitude of 1. \n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) / self.tau   \n",
    "        \n",
    "        \n",
    "        if self.relative_pos_embedding:\n",
    "            tmp1 = self.relative_indices[:, :, 0] # the 1st channel is taken \n",
    "            # tmp2 = self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]] # (49, 49, 2)\n",
    "            \n",
    "            \n",
    "            # indexing with and adding the elements to the \"dots\", acessing from the  \n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            \"\"\"Softmax(QK_t / sqrt(d) + B)*V \"\"\" # B - pos embedding \n",
    "            # pos_embedding - (49, 49)\n",
    "            # dots - b h w i j\n",
    "            dots += self.pos_embedding # added to every window (w) of size (49, 49) that is (i, j)\n",
    "\n",
    "        \"\"\"We need to add masking to last row and last column as we used cyclic_shift\"\"\"\n",
    "        \n",
    "        if self.shifted:\n",
    "            # tmp1 = self.upper_lower_mask\n",
    "            # tmp2 = self.left_right_mask\n",
    "            \"\"\"we are applying to all the windows [3] dim \"\"\" \n",
    "            # dots - (b, h, w, i, j) # to all the heads and to only the last row and last column windows   \n",
    "            # no of windows        # only to windows in the last row and last column \n",
    "            dots[:, :, -nw_w] += self.upper_lower_mask  # just slice the last row by -nw_w  # nw_w - no of windows ( n_w x n_w )\n",
    "            # mask only the last column windows \n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask # add to the last column\n",
    "            # for the first stage, nw_w = 8, so, nw_w - 1 = 7 \n",
    "            \"\"\"[start:stop:step] in slicing\"\"\" # so, 7::8 # means do, till the last\n",
    "            # mapping only the last column.  \n",
    "        \n",
    "        attn = dots.softmax(dim=-1) # across all the windows # \n",
    "        # (b, h=(3, 6, 12, 24), (nw_h * nw_w)=(64, 16, 4, 1), (w_h*w_w)=49, d=32) where d=head_dim, h=#heads \n",
    "        \n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        # rearrage the output as to the original input to the \"WindowAttention\" as the widnow attention wouldn't change the dim or the feature size, only the PatchMerging would do that\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
    "        \n",
    "        out = self.to_out(out)  # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
    "        # if we are in shifted window, we need to cycle back to get the original feature set \n",
    "        \n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        \n",
    "        return out        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the SWIN Transformer v2, they have used Cosine Similarity instead of the Dot product similarity \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3, 2])\n",
      "tensor([[1, 1, 1],\n",
      "        [4, 4, 4],\n",
      "        [2, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "# data matrix to index \n",
    "p = torch.tensor([[1, 2], \n",
    "                  [3, 4]])\n",
    "\n",
    "print(p.size())\n",
    "\n",
    "# indices \n",
    "r = torch.tensor([[[0, 0], [0,0], [0,0]],\n",
    "                 [[1,1], [1,1], [1,1]],\n",
    "                 [[0,1], [0,1], [0,1]]])  # 3, 3, 2\n",
    "print(r.size())\n",
    "\n",
    "# indexing the tensor with another tensor \n",
    "print(p[r[:, :, 0], r[:, :, 1]])  # great work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 0.3016, -0.1073,  0.9985, -0.4987,  0.7611],\n",
      "           [ 0.6183, -0.2994, -0.1878, -0.1201,  0.3605],\n",
      "           [-0.3140, -1.0787,  0.2408, -1.3962,  0.1136],\n",
      "           [ 1.1047, -1.5616, -0.3546,  1.0811,  0.1315],\n",
      "           [ 1.5735,  0.7814,  0.9874, -1.4878,  1.4708]],\n",
      "\n",
      "          [[ 0.2756,  0.6668, -0.9944, -1.1894, -1.1959],\n",
      "           [ 1.3119, -0.2098,  0.4069,  0.3946, -1.2537],\n",
      "           [ 0.9868, -0.4947, -1.2830,  0.4386, -0.0107],\n",
      "           [ 1.3384, -0.2794,  0.2877, -0.0334, -1.0619],\n",
      "           [-0.1144,  0.1954, -0.7371,  1.7001,  0.3462]]]]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[[[[ 1.3016,  0.8927,  1.9985,  0.5013,  1.7611],\n",
      "           [ 1.6183,  0.7006,  0.8122,  0.8799,  1.3605],\n",
      "           [ 0.6860, -0.0787,  1.2408, -0.3962,  1.1136],\n",
      "           [ 2.1047, -0.5616,  0.6454,  2.0811,  1.1315],\n",
      "           [ 2.5735,  1.7814,  1.9874, -0.4878,  2.4708]],\n",
      "\n",
      "          [[ 1.2756,  1.6668,  0.0056, -0.1894, -0.1959],\n",
      "           [ 2.3119,  0.7902,  1.4069,  1.3946, -0.2537],\n",
      "           [ 1.9868,  0.5053, -0.2830,  1.4386,  0.9893],\n",
      "           [ 2.3384,  0.7206,  1.2877,  0.9666, -0.0619],\n",
      "           [ 0.8856,  1.1954,  0.2629,  2.7001,  1.3462]]]]])\n"
     ]
    }
   ],
   "source": [
    "# for dots and pos_embed adding \n",
    "do = torch.randn(1, 1, 2, 5, 5) # 2 windows  \n",
    "print(do)\n",
    "\n",
    "po = torch.ones(5, 5) # pos_embedding\n",
    "print(po)\n",
    "\n",
    "su = do + po\n",
    "print(su)\n",
    "\n",
    "# 1 is added to each of the element and added to all the windows \n",
    "# so, pos_embedding is added to every windows ( 7 x 7) and it is done to all the windows in the feature set as the feature set is divided into multiple windows \n",
    "\n",
    "#----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3  4  5  6  7]\n",
      "  [ 8  9 10 11 12 13 14 15]\n",
      "  [16 17 18 19 20 21 22 23]\n",
      "  [24 25 26 27 28 29 30 31]\n",
      "  [32 33 34 35 36 37 38 39]\n",
      "  [40 41 42 43 44 45 46 47]\n",
      "  [48 49 50 51 52 53 54 55]\n",
      "  [56 57 58 59 60 61 62 63]]]\n",
      "[[[ 7]\n",
      "  [15]\n",
      "  [23]\n",
      "  [31]\n",
      "  [39]\n",
      "  [47]\n",
      "  [55]\n",
      "  [63]]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(64).reshape(1, 8, 8)  # batch, 8 x8 for stage 1\n",
    "print(arr)\n",
    "\n",
    "result = arr[:, :, 7::8]  # all the rows, and last column \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[-0.1448,  0.6376, -0.2813, -1.3299, -0.6538],\n",
      "           [ 1.7198, -0.9610, -0.6375, -0.8870,  0.8388],\n",
      "           [ 1.1529, -1.7611, -1.1070, -1.7174,  1.5346],\n",
      "           [-0.0032,  1.4403, -0.1106,  0.5769, -0.1692],\n",
      "           [ 1.1887, -0.1575, -0.0455,  0.6485, -0.8707]],\n",
      "\n",
      "          [[ 0.1447,  1.9029,  0.3904,  0.0331, -1.0234],\n",
      "           [ 0.7335,  1.1177,  0.5851, -1.1560, -0.5354],\n",
      "           [-0.8637, -0.9069, -0.5918,  0.1508, -1.0411],\n",
      "           [-0.7205, -2.2148,  0.9403, -1.1470,  0.7928],\n",
      "           [ 0.0832,  0.4228, -1.8687, -1.1057,  0.1437]]]]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[[[[-0.1448,  0.6376, -0.2813, -1.3299, -0.6538],\n",
      "           [ 1.7198, -0.9610, -0.6375, -0.8870,  0.8388],\n",
      "           [ 1.1529, -1.7611, -1.1070, -1.7174,  1.5346],\n",
      "           [-0.0032,  1.4403, -0.1106,  0.5769, -0.1692],\n",
      "           [ 1.1887, -0.1575, -0.0455,  0.6485, -0.8707]],\n",
      "\n",
      "          [[ 1.1447,  2.9029,  1.3904,  1.0331, -0.0234],\n",
      "           [ 1.7335,  2.1177,  1.5851, -0.1560,  0.4646],\n",
      "           [ 0.1363,  0.0931,  0.4082,  1.1508, -0.0411],\n",
      "           [ 0.2795, -1.2148,  1.9403, -0.1470,  1.7928],\n",
      "           [ 1.0832,  1.4228, -0.8687, -0.1057,  1.1437]]]]])\n"
     ]
    }
   ],
   "source": [
    "do = torch.randn(1, 1, 2, 5, 5)  # 2 windows of size 5 x 5 \n",
    "print(do)\n",
    "\n",
    "mo = torch.ones(5, 5) # to add with windows \n",
    "print(mo)\n",
    "\n",
    "do[:, :, 1] += mo  # applies to only the last window \n",
    "print(do) # pos embedding is added only to the 2nd column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # hidden_dim --- 4 times the size of the input (channels)\n",
    "        self.net = nn.Sequential(\n",
    "            # linear layers \n",
    "            nn.Linear(dim, hidden_dim),  \n",
    "            # activation function  \n",
    "            nn.GELU(),  # Gaussian Error Linear Units\n",
    "            nn.Linear(hidden_dim, dim),  # process with hidden_dim and bring back to the same dim after processing \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # FFC 2 layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine similarity \n",
    "\n",
    "Doing the normalization stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [2., 2.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 4.],\n",
      "         [3., 6.],\n",
      "         [5., 6.]],\n",
      "\n",
      "        [[9., 1.],\n",
      "         [2., 8.],\n",
      "         [9., 7.]]])\n",
      "torch.Size([3, 3, 2])\n",
      "tensor([[1., 2., 1.],\n",
      "        [1., 3., 5.],\n",
      "        [9., 2., 9.]]) tensor([[2., 2., 1.],\n",
      "        [4., 6., 6.],\n",
      "        [1., 8., 7.]])\n",
      "tensor([[[0.4472, 0.8944],\n",
      "         [0.7071, 0.7071],\n",
      "         [0.7071, 0.7071]],\n",
      "\n",
      "        [[0.2425, 0.9701],\n",
      "         [0.4472, 0.8944],\n",
      "         [0.6402, 0.7682]],\n",
      "\n",
      "        [[0.9939, 0.1104],\n",
      "         [0.2425, 0.9701],\n",
      "         [0.7894, 0.6139]]])\n",
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[1., 2], [2, 2], [1, 1]],\n",
    "                  [[1, 4], [3, 6], [5, 6]],\n",
    "                  [[9, 1], [2, 8], [9, 7]]])\n",
    "print(a) \n",
    "print(a.size())\n",
    "print(a[:, :, 0], a[:, :, 1])\n",
    "# p = exponent value of the norm function \n",
    "a = f.normalize(a, p=2, dim=-1)\n",
    "print(a)\n",
    "print(a.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swin Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output and input of the SWIN transformer block is the same \n",
    "# after patch merging, the feature set is sent to the STB, and then the input and the output of the STB would be the same, and then, \n",
    "# patch merging will again reduce the size of th feature\n",
    "\n",
    "# so, PM is the one down-scaling the feature \n",
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, \n",
    "                 window_size, relative_pos_embedding):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Residual connection after the layer normalization and the Attention\n",
    "        self.attention_block = Residual( # Residual on the whole block... \n",
    "            PostNorm(  # Layer normalization, we send the dim size and after computoing the attention\n",
    "                dim,   # which dim of the input to the layer norm \n",
    "                \n",
    "                WindowAttention(  # HOW IT WORKS\n",
    "                    dim=dim, \n",
    "                    heads=heads, \n",
    "                    head_dim=head_dim, \n",
    "                    window_size=window_size, \n",
    "                    shifted=shifted,\n",
    "                    relative_pos_embedding=relative_pos_embedding\n",
    "                )\n",
    "            )\n",
    "        ) # MLP is the FeedForward   # we do residual connection and before that in the block, we have LN (Layer Normalization)\n",
    "        \"\"\"\n",
    "        W-MSA: \n",
    "            [LN, W-MSA] -> [LN, MLP]\n",
    "        SW-MSA:  \n",
    "            [LN, SW-MSA] -> [LN, MLP]\n",
    "\n",
    "        \"\"\"\n",
    "        # FeedForward after the first block according to the block in the paper \n",
    "        self.mlp_block = Residual(PostNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "        # residual -> PreNorm -> FeedForward\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)  \n",
    "        x = self.mlp_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 hidden_dimension, \n",
    "                 layers, \n",
    "                 downscaling_factor, \n",
    "                 num_heads, \n",
    "                 head_dim, \n",
    "                 window_size, \n",
    "                 relative_pos_embedding\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # for SwinTrans-T: {2, 2, 6, 2} for 4 stage blocks \n",
    "        # so, we atleast need 2 sub-blocks of a stage block, so, it should be in even \n",
    "        assert layers % 2 == 0 # stage layers need to be divisible by 2 for regular and shifted block \n",
    "        \n",
    "        # Patch Partition  --  done before the 4 stages \n",
    "        # Dones at every stage \n",
    "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension, \n",
    "                                            downscaling_factor=downscaling_factor)\n",
    "        \n",
    "        # Stage 1: LE + STB   # only first stage has Linear Embedding \n",
    "        # Stage 2: PM + STB\n",
    "        # Stage 3: PM + STB\n",
    "        # Stage 4: PM + STB\n",
    "        self.layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(layers // 2):  # we will go only one iteration. we will use the same n times, so the paramters don't get added up \n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        # W-MSA\n",
    "                        SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                              shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),  # stage = False\n",
    "                        # SW-MSA\n",
    "                        SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                              shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding)                        \n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        # path merging \n",
    "        print('Before path merging:', x.size())   # (1 batch_size, (3, 96, 192, 384) channels, (3, 224, 56, 28, 14) height, (224, 56, 28, 14) width)\n",
    "        x = self.patch_partition(x)  # it will down scale the feature image \n",
    "        print('After patch merging:', x.size())\n",
    "        # input and output of the SWIN transformer block is the same size \n",
    "        # patch_merging is the one which reduces the size of the feature image \n",
    "        \n",
    "        \"\"\"\n",
    "        For the 1st stage, 2 blocks -> Patch Partition and Linear Embedding \n",
    "        (just resizes the feature)\n",
    "        # In paper code, they used only one block for the stage 1 instead of 2 \n",
    "        \"\"\"\n",
    "        # just pass through all the blocks \n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            \n",
    "            # these two blocks, W-MSA and SW-MSA won't change the size of th feature    \n",
    "            x = regular_block(x)\n",
    "            x = shifted_block(x)\n",
    "        \n",
    "        return x.permute(0, 3, 1, 2) # batch, channels, h, w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *, \n",
    "        hidden_dim, \n",
    "        layers, \n",
    "        heads, \n",
    "        channels=3, \n",
    "        num_classes=1000,   # for Imagenet as used in the paper \n",
    "        head_dim=32, \n",
    "        window_size=7, \n",
    "        downscaling_factors=(4, 2, 2, 2), \n",
    "        relative_pos_embedding=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        As we go forward with 4 stages, the size of the image is shrinking and the channels goes on increasing\n",
    "        \"\"\"\n",
    "        # for processing the information in the image\n",
    "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
    "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        \n",
    "        # downscaluing_factor for the patch_merging \n",
    "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
    "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        \n",
    "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
    "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        \n",
    "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
    "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        \n",
    "        # classification or decision head \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim * 8),\n",
    "            nn.Linear(hidden_dim * 8, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \n",
    "        x = self.stage1(img)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)      # (batch, 768, 7, 7) --- 4 dim \n",
    "        \n",
    "        # ?????\n",
    "        x = x.mean(dim=[2, 3])     # (batch, 1, 768)    --- 3 dim --> 2 dim without batch \n",
    "        return self.mlp_head(x)  # give the linear form   # categrotical output \n",
    "    \n",
    "    \"\"\"this mlp_head could be changed to generate the text or LLM output and we can also embed the XAI into this.. \n",
    "    Possible\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWIN- \n",
    "\n",
    "T: C=96, layers = {2, 2, 6, 2}\n",
    "\n",
    "S: C=96, layers = {2, 2, 18, 2}\n",
    "\n",
    "B: C=128, layers = {2, 2, 18, 2}\n",
    "\n",
    "L: C=192, layers = {2, 2, 18, 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_t(\n",
    "    hidden_dim=96,  # no of channels at begning and then 2C, 4C, and 8C \n",
    "    layers=(2, 2, 6, 2),  # in the paper \n",
    "    heads=(3, 6, 12, 24),\n",
    "    **kwargs\n",
    "):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "def swin_s(\n",
    "    hidden_dim=96, \n",
    "    layers=(2, 2, 18, 2),   \n",
    "    heads=(3, 6, 12, 24),   \n",
    "    **kwargs\n",
    "):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "def swin_b(\n",
    "    hidden_dim=128, \n",
    "    layers=(2, 2, 18, 2), \n",
    "    heads=(4, 8, 16, 32),\n",
    "    **kwargs\n",
    "):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "def swin_l(\n",
    "    hidden_dim=192,  \n",
    "    layers=(2, 2, 18, 2),  \n",
    "    heads=(6, 12, 24, 48),\n",
    "    **kwargs\n",
    "):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformer(\n",
      "  (stage1): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
      "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate='none')\n",
      "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
      "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate='none')\n",
      "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage2): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (1): GELU(approximate='none')\n",
      "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (1): GELU(approximate='none')\n",
      "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage3): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU(approximate='none')\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU(approximate='none')\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage4): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (1): GELU(approximate='none')\n",
      "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PostNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (1): GELU(approximate='none')\n",
      "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = swin_t(\n",
    "    hidden_dim=96, # start channel \n",
    "    layers=(2, 2, 6, 2), # 4 stages\n",
    "    heads=(3, 6, 12, 24),\n",
    "    channels=3, \n",
    "    num_classes=3,\n",
    "    head_dim=32,\n",
    "    window_size=7,\n",
    "    downscaling_factors=(4, 2, 2, 2),\n",
    "    relative_pos_embedding=True\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before path merging: torch.Size([1, 3, 224, 224])\n",
      "After patch merging: torch.Size([1, 56, 56, 96])\n",
      "Before path merging: torch.Size([1, 96, 56, 56])\n",
      "After patch merging: torch.Size([1, 28, 28, 192])\n",
      "Before path merging: torch.Size([1, 192, 28, 28])\n",
      "After patch merging: torch.Size([1, 14, 14, 384])\n",
      "Before path merging: torch.Size([1, 384, 14, 14])\n",
      "After patch merging: torch.Size([1, 7, 7, 768])\n",
      "tensor([[ 0.3548, -0.2833, -0.5480]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_x = torch.randn(1, 3, 224, 224)  # dummy input image tensor \n",
    "logits = net(dummy_x) # (1, 3) # as it would return the softmax \n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
